{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c40644c2-96cf-4fbf-a684-15a0e0fcf52d",
   "metadata": {},
   "source": [
    "<font size=\"5\">DATA SCRAPER</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc217342-4223-4617-b038-45fa01cc8200",
   "metadata": {},
   "source": [
    "This Jupyter notebook is used to download, pre-process, and save the job-offer data from the website https://nofluffjobs.com/cz/.  The downloaded data will be exported into the CSV file, which will be further processed and cleaned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199c1648-3a68-4289-801d-a4fc3e30d6e2",
   "metadata": {},
   "source": [
    "<font size=\"4\">Import of packages</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef73dcce-754a-4513-84bb-055ce89c91af",
   "metadata": {},
   "source": [
    "We are importing the os package to check if we are in the correct directory.\n",
    "\n",
    "The packages requests and BeautifulSoup will be utilized to scrape the data from the website and the time package will set up short pauses during the web scraping process. The tqdm package will provide us with an interactive measurement of the downloading progress and pandas library will enable us to organize our scraped data into a table format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb3b399-0a1d-4c12-aa1f-e3eb22bbf670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /Users/leducanh/Documents/pythonProject\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "945111ec-f79e-4cf1-af3c-acb7c041af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc3e6c-2b2f-4a90-84de-ee060f8ff6d5",
   "metadata": {},
   "source": [
    "<font size=\"4\">Data scraping</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd67a85-2c5a-45f5-8036-b6e79bc93eca",
   "metadata": {},
   "source": [
    "We will create the Scraper class, which will help us to download the data effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c416d1-7667-4aa6-be0e-728d10248f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper:\n",
    "    def __init__(self, mainUrl):\n",
    "        self.links = {\n",
    "            'categories': [],\n",
    "            'pages': {},\n",
    "            'jobOffers': []\n",
    "        }\n",
    "        self.data = {\n",
    "            'jobTitles': [],\n",
    "            'company': [],\n",
    "            'salaryRange': [],\n",
    "            'Bonus': [],\n",
    "            'category+progLang': [],\n",
    "            'seniority': [],\n",
    "            'workLanguage': [],\n",
    "            'location': [],\n",
    "            'remote':[]\n",
    "        }\n",
    "        self.mainUrl = mainUrl\n",
    "        self.categList = []\n",
    "\n",
    "    # getting the links for job categories stored in slef.links['categories']\n",
    "    def getUrlByCat(self):\n",
    "\n",
    "        response = requests.get(self.mainUrl)\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'lxml')\n",
    "        linksByCat = soup.find_all('div',\n",
    "                                   class_=\"d-flex justify-content-between align-items-center list-title-wrapper ng-star-inserted\")\n",
    "        \n",
    "        urlList = [self.mainUrl + href.a['href'] for href in linksByCat[1:]]\n",
    "        \n",
    "        # removing redundant links in urlList\n",
    "        self.links['categories'] = [link for link in urlList if '?criteria' not in link]\n",
    "\n",
    "        #get a list of job categories, which will be used as keys in the links['pages'], links['jobOffers'] dictionary\n",
    "        self.categList = [category.rsplit('/', 1)[-1] for category in self.links['categories']]\n",
    "        print('categorical urls success')\n",
    "    \n",
    "    # method to get all pages' urls for each it category (e.g. backend, frontend) stored in self.links['pages']\n",
    "    def getPages(self):\n",
    "        \n",
    "        '''\n",
    "        fill the self.links['pages'], where categories are denoted as keys with the value being a list containing\n",
    "                 the link to the first page of job offers of the particular category\n",
    "                 later, the idea will be to append these lists, with the links to the next job offer pages\n",
    "                 '''\n",
    "\n",
    "        self.links['pages'] = {k: [v] for v, k in zip(self.links['categories'], self.categList)}\n",
    "\n",
    "        '''\n",
    "        e.g. links['pages'] = {'backend': ['https://nofluffjobs.com/cz/it-prace/backend'], \n",
    "                                  'frontend': ['https://nofluffjobs.com/cz/it-prace/frontend'],\n",
    "                                   ...}\n",
    "                                   '''\n",
    "\n",
    "        # adding the page url from the next button (first url page is already added)\n",
    "        for url in self.links['pages'].values():\n",
    "            while True:\n",
    "                try:\n",
    "                    response = requests.get(url[-1])\n",
    "                    soup = BeautifulSoup(response.text, 'lxml')\n",
    "                    pageLink = self.mainUrl + soup.find('a', {'aria-label': 'Next'}).get('href')\n",
    "                    url.append(pageLink)\n",
    "                except AttributeError:\n",
    "                    break\n",
    "                time.sleep(0.2)\n",
    "                response = requests.get(pageLink)\n",
    "                #soup = BeautifulSoup(response.text, 'lxml')\n",
    "        print('page urls success')\n",
    "    \n",
    "    # get the urls for all job offers stored in self.links['jobOffers']\n",
    "    def getJobLinks(self):\n",
    "        \n",
    "        '''\n",
    "        add keys based on job categories to the self.links['jobOffers'] dictionary  ,the values are emtpy lists\n",
    "        self.links['jobOffers'] = {k: [] for k in self.categList}\n",
    "        pageLinks is a dictionary with key: list values, where key is the job category and lists contain links for the pages\n",
    "        '''\n",
    "        \n",
    "        for lists in self.links['pages'].values():\n",
    "            for link in lists:\n",
    "                time.sleep(0.1)\n",
    "                # request applied on a particular page\n",
    "                response = requests.get(link).text\n",
    "                soup = BeautifulSoup(response, 'lxml')\n",
    "                #select all the 'a' tags on the particular page with the idea to capture all the 'hrefs' from there\n",
    "                aTags = soup.select('body > nfj-root > nfj-layout > nfj-main-content > div > nfj-postings-search > div > common-main-loader > div > nfj-search-results > nfj-postings-list > div.list-container.ng-star-inserted>a')\n",
    "                \n",
    "                '''\n",
    "                looping through the 'a' tags to capture the 'hrefs' which are than added into the 'jobOffers' dictionary's list\n",
    "                keys denotes the job category, where the particular 'href' belongs to \n",
    "                '''\n",
    "                \n",
    "                for href in aTags:\n",
    "                    self.links['jobOffers'].append(self.mainUrl + href.get('href'))\n",
    "        print('job sites url success')\n",
    "        \n",
    "    def getData(self):\n",
    "        for link in tqdm(self.links['jobOffers']):    \n",
    "            time.sleep(0.2)\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            #getting job titles\n",
    "            jobTitle = soup.find('div',\n",
    "                                 class_=\"posting-details-description d-flex align-items-center align-items-lg-start flex-column justify-content-center justify-content-lg-start\")\n",
    "            try:\n",
    "                self.data['jobTitles'].append(jobTitle.h1.text)\n",
    "            except:\n",
    "                self.data['jobTitles'].append(None)\n",
    "            self.data['company'].append(jobTitle.a.text)\n",
    "\n",
    "            #getting offered salary ranges\n",
    "            salaryRange = soup.find('div', class_=\"salary\")\n",
    "            self.data['salaryRange'].append(salaryRange.h4.text)\n",
    "\n",
    "            #getting  bonus\n",
    "            bonus = soup.find('common-postings-bonus')\n",
    "            try:\n",
    "                self.data['Bonus'].append(bonus.a.text)\n",
    "            # if no bonus than assign None\n",
    "            except:\n",
    "                self.data['Bonus'].append(None)\n",
    "\n",
    "            #getting job's location\n",
    "            location = soup.find('common-posting-locations')\n",
    "            try:\n",
    "                self.data['location'].append(location.text)\n",
    "            except:\n",
    "                self.data['location'].append(None)\n",
    "            \n",
    "            #getting the info, whether it is a remote job\n",
    "            remote = soup.find('li', class_=\"remote\")\n",
    "            try:\n",
    "                self.data['remote'].append(remote.text)\n",
    "            except:\n",
    "                self.data['remote'].append(None)\n",
    "            \n",
    "            # getting the job category and programming language\n",
    "            # [0] to unlist the soup.select\n",
    "            categoryProgLang = soup.select('body > nfj-root > nfj-layout > nfj-main-content > div > nfj-posting-details > common-main-loader > main > article > div.col.mobile-no-padding > common-posting-content-wrapper > div.border > section> ul > li:nth-child(1)')[0]\n",
    "            self.data['category+progLang'].append(categoryProgLang.text)\n",
    "            \n",
    "            #getting the seniority level\n",
    "            seniority = soup.select('body > nfj-root > nfj-layout > nfj-main-content > div > nfj-posting-details > common-main-loader > main > article > div.col.mobile-no-padding > common-posting-content-wrapper > div.border > section> ul > li:nth-child(2)')[0]\n",
    "            self.data['seniority'].append(seniority.text)\n",
    "            \n",
    "            #getting the working language\n",
    "            workLang = soup.select('body > nfj-root > nfj-layout > nfj-main-content > div > nfj-posting-details > common-main-loader > main > article > div.col.mobile-no-padding > common-posting-content-wrapper > div.border > section> ul > li:nth-child(3)')[0]\n",
    "            try:\n",
    "                self.data['workLanguage'].append(workLang.text)\n",
    "            except:\n",
    "                self.data['workLanguage'].append(None)\n",
    "        print('data extraction success')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07feb7a-e48f-449e-9f07-20f1a1e2a82b",
   "metadata": {},
   "source": [
    "Creating the Sraper class object to download the data from the website. Tqdm package is utilized to give us info about the downloading process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7eb3627d-f63e-43b9-9246-6abe402813b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categorical urls success\n",
      "page urls success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/52 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job sites url success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 52/52 [00:54<00:00,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data extraction success\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mainUrl = 'https://nofluffjobs.com'\n",
    "\n",
    "site = Scraper(mainUrl)\n",
    "\n",
    "site.getUrlByCat()\n",
    "\n",
    "site.getPages()\n",
    "\n",
    "site.getJobLinks()\n",
    "\n",
    "site.getData()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c907c127-f493-4d9f-87f9-e7084fcf7b23",
   "metadata": {},
   "source": [
    "<font size=\"4\">Pre-processing and saving the data</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22313e3-b841-446d-8c88-38db13ea63fb",
   "metadata": {},
   "source": [
    "We convert the dictionary from the Scraper class object into the dataframe, which is then saved into the csv file using the pandas functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9843d93-3ab6-4f2f-b3d3-90316807a345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(site.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8873340b-51c7-4b0d-9a2b-ecdf34933c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>jobTitles</th>\n",
       "      <th>company</th>\n",
       "      <th>salaryRange</th>\n",
       "      <th>Bonus</th>\n",
       "      <th>category+progLang</th>\n",
       "      <th>seniority</th>\n",
       "      <th>workLanguage</th>\n",
       "      <th>location</th>\n",
       "      <th>remote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NodeJS Backend Developer üî•</td>\n",
       "      <td>Dayswaps</td>\n",
       "      <td>67.2k  - 117.6k  CZK</td>\n",
       "      <td>10k CZK</td>\n",
       "      <td>Kategorie:  Backend ,¬† node</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Pracovn√≠ jazyk:ƒåe≈°tina, Sloven≈°tina</td>\n",
       "      <td>Hybrid  +1 ‚Ä¢Prague, Na Zderaze 5</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PHP Developer</td>\n",
       "      <td>Lentiamo s.r.o.</td>\n",
       "      <td>50k  - 90k  CZK</td>\n",
       "      <td>None</td>\n",
       "      <td>Kategorie:  Backend ,¬† php</td>\n",
       "      <td>Mid, Senior</td>\n",
       "      <td>Pracovn√≠ jazyk:ƒåe≈°tina, Angliƒçtina</td>\n",
       "      <td>Hybrid  +2 ‚Ä¢Prague, ≈†panƒõlsk√° 770/2‚Ä¢Velk√© Mez...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Backend Developer</td>\n",
       "      <td>PECOSTA, a.s.</td>\n",
       "      <td>60k  - 90k  CZK</td>\n",
       "      <td>None</td>\n",
       "      <td>Kategorie:  Backend ,¬† .net</td>\n",
       "      <td>Mid, Senior</td>\n",
       "      <td>Pracovn√≠ jazyk:ƒåe≈°tina</td>\n",
       "      <td>Hybrid  +1 ‚Ä¢Ostrava, Nemocniƒçn√≠ 987/12</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PHP Developer</td>\n",
       "      <td>Papirfly</td>\n",
       "      <td>50k  - 100k  CZK</td>\n",
       "      <td>None</td>\n",
       "      <td>Kategorie:  Backend ,¬† php</td>\n",
       "      <td>Mid</td>\n",
       "      <td>Pracovn√≠ jazyk:ƒåe≈°tina, Angliƒçtina</td>\n",
       "      <td>Brno, Zl√≠n ‚Ä¢Brno‚Ä¢Zl√≠n</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kotlin Developer</td>\n",
       "      <td>Onlio, a.s.</td>\n",
       "      <td>46.2k  - 92.4k  CZK</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "      <td>Mid</td>\n",
       "      <td>Pracovn√≠ jazyk:ƒåe≈°tina</td>\n",
       "      <td>Hybrid  +1 ‚Ä¢Praha 7, U gar√°≈æ√≠ 1611/1 40% na m...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      jobTitles            company             salaryRange  \\\n",
       "0   NodeJS Backend Developer üî•           Dayswaps    67.2k  - 117.6k  CZK    \n",
       "1                PHP Developer    Lentiamo s.r.o.         50k  - 90k  CZK    \n",
       "2            Backend Developer      PECOSTA, a.s.         60k  - 90k  CZK    \n",
       "3                PHP Developer           Papirfly        50k  - 100k  CZK    \n",
       "4             Kotlin Developer        Onlio, a.s.     46.2k  - 92.4k  CZK    \n",
       "\n",
       "     Bonus              category+progLang      seniority  \\\n",
       "0  10k CZK   Kategorie:  Backend ,¬† node            Mid    \n",
       "1     None    Kategorie:  Backend ,¬† php    Mid, Senior    \n",
       "2     None   Kategorie:  Backend ,¬† .net    Mid, Senior    \n",
       "3     None    Kategorie:  Backend ,¬† php            Mid    \n",
       "4     None                                          Mid    \n",
       "\n",
       "                          workLanguage  \\\n",
       "0  Pracovn√≠ jazyk:ƒåe≈°tina, Sloven≈°tina   \n",
       "1   Pracovn√≠ jazyk:ƒåe≈°tina, Angliƒçtina   \n",
       "2               Pracovn√≠ jazyk:ƒåe≈°tina   \n",
       "3   Pracovn√≠ jazyk:ƒåe≈°tina, Angliƒçtina   \n",
       "4               Pracovn√≠ jazyk:ƒåe≈°tina   \n",
       "\n",
       "                                            location remote  \n",
       "0                  Hybrid  +1 ‚Ä¢Prague, Na Zderaze 5    None  \n",
       "1   Hybrid  +2 ‚Ä¢Prague, ≈†panƒõlsk√° 770/2‚Ä¢Velk√© Mez...   None  \n",
       "2            Hybrid  +1 ‚Ä¢Ostrava, Nemocniƒçn√≠ 987/12    None  \n",
       "3                             Brno, Zl√≠n ‚Ä¢Brno‚Ä¢Zl√≠n    None  \n",
       "4   Hybrid  +1 ‚Ä¢Praha 7, U gar√°≈æ√≠ 1611/1 40% na m...   None  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2f4784f-131a-4a5f-ad6a-8df06cc2240c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('jobOffers-24.08.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
